# <center>《计算机组成原理》矩阵乘法实验报告</center>

<h5 align="center">实验名称：矩阵乘法   班级：李涛老师  学生姓名：陆皓喆  学号：2211044</h5 align="center">

<h5 align="center">指导老师：董前琨    实验地点：实验楼A306  实验时间：2024.04.21</h5 align="center">



# 一、实验要求

## 1.1 个人PC电脑实验

个人`PC`电脑实验要求如下：

1. 使用个人电脑完成，不仅限于`visual studio`、`vscode`等。

2. 在完成矩阵乘法优化后，测试矩阵规模在$1024-4096$，或更大维度上，至少进行$4$个矩阵规模维度的测试。如`PC`电脑有`Nvidia`显卡，建议尝试`CUDA`代码。

3. 在作业中需总结出不同层次，不同规模下的矩阵乘法优化对比，对比指标包括计算耗时、运行性能、加速比等。

4. 在作业中总结优化过程中遇到的问题和解决方式。




## 1.2 Taishan服务器实验

在`Taishan`服务器上使用`vim+gcc`编程环境，要求如下：

1. 在`Taishan`服务器上完成，使用`Putty`等远程软件在校内登录使用，服务器IP：`222.30.62.23`，端口`22`，用户名`stu+学号`，默认密码`123456`，登录成功后可自行修改密码。

2. 在完成矩阵乘法优化后（使用$AVX$库进行子字优化在`Taishan`服务器上的软件包环境不好配置，可以不进行此层次优化操作，注意原始代码需要调整），测试矩阵规模在$1024-4096$，或更大维度上，至少进行$4$个矩阵规模维度的测试。

3. 在作业中需总结出不同层次，不同规模下的矩阵乘法优化对比，对比指标包括计算耗时、运行性能、加速比等。

4. 在作业中需对比`Taishan`服务器和自己个人电脑上程序运行时间等相关指标，分析一下不同电脑上的运行差异的原因，总结在优化过程中遇到的问题和解决方式。



# 二、个人PC电脑的实验

## 2.1 源代码

```c++
#include<iostream>
#include<time.h>
//#include<x86intrin.h>
#include<immintrin.h>

using namespace std;
#define REAL_T double 

void printFlops(int A_height, int B_width, int B_height, clock_t start, clock_t stop) {
	REAL_T flops = (2.0 * A_height * B_width * B_height) / 1E9 / ((stop - start) / (CLOCKS_PER_SEC * 1.0));
	cout << "GFLOPS:\t" << flops << endl;
}

void initMatrix(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
	for (int i = 0; i < n; ++i)
		for (int j = 0; j < n; ++j) {
			A[i + j * n] = (i + j + (i * j) % 100) % 100;
			B[i + j * n] = ((i - j) * (i - j) + (i * j) % 200) % 100;
			C[i + j * n] = 0;
		}
}

void dgemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
	for (int i = 0; i < n; ++i)
		for (int j = 0; j < n; ++j) {
			REAL_T cij = C[i + j * n];
			for (int k = 0; k < n; k++) {
				cij += A[i + k * n] * B[k + j * n];
			}
			C[i + j * n] = cij;
		}
}

void avx_dgemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
	for (int i = 0; i < n; i += 4)
		for (int j = 0; j < n; ++j) {
			__m256d cij = _mm256_load_pd(C + i + j * n);
			for (int k = 0; k < n; k++) {
				//cij += A[i+k*n] * B[k+j*n];
				cij = _mm256_add_pd(
					cij,
					_mm256_mul_pd(_mm256_load_pd(A + i + k * n), _mm256_load_pd(B + i + k * n))
				);
			}
			_mm256_store_pd(C + i + j * n, cij);
		}
}

#define UNROLL (4)

void pavx_dgemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
	for (int i = 0; i < n; i += 4 * UNROLL)
		for (int j = 0; j < n; ++j) {
			__m256d cij[4];
			for (int x = 0; x < UNROLL; ++x)
				cij[x] = _mm256_load_pd(C + i + j * n);

			for (int k = 0; k < n; k++) {
				//cij += A[i+k*n] * B[k+j*n];
				/*cij = _mm256_add_pd(
						cij,
						_mm256_mul_pd( _mm256_load_pd(A+i+k*n),  _mm256_load_pd(B+i+k*n) )
						);*/
				__m256d b = _mm256_broadcast_sd(B + k + j * n);
				for (int x = 0; x < UNROLL; ++x)
					cij[x] = _mm256_add_pd(
						cij[x],
						_mm256_mul_pd(_mm256_load_pd(A + i + 4 * x + k * n), b));
			}
			for (int x = 0; x < UNROLL; ++x)
				_mm256_store_pd(C + i + x * 4 + j * n, cij[x]);
		}
}

#define BLOCKSIZE (32)
void do_block(int n, int si, int sj, int sk, REAL_T* A, REAL_T* B, REAL_T* C) {
	for (int i = si; i < si + BLOCKSIZE; i += UNROLL * 4)
		for (int j = sj; j < sj + BLOCKSIZE; ++j) {
			__m256d c[4];
			for (int x = 0; x < UNROLL; ++x)
				c[x] = _mm256_load_pd(C + i + 4 * x + j * n);

			for (int k = sk; k < sk + BLOCKSIZE; ++k) {
				__m256d b = b = _mm256_broadcast_sd(B + k + j * n);
				for (int x = 0; x < UNROLL; ++x)
					c[x] = _mm256_add_pd(
						c[x],
						_mm256_mul_pd(_mm256_load_pd(A + i + 4 * x + k * n), b));
			}

			for (int x = 0; x < UNROLL; ++x)
				_mm256_store_pd(C + i + x * 4 + j * n, c[x]);
		}
}


void block_gemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
	for (int sj = 0; sj < n; sj += BLOCKSIZE)
		for (int si = 0; si < n; si += BLOCKSIZE)
			for (int sk = 0; sk < n; sk += BLOCKSIZE)
				do_block(n, si, sj, sk, A, B, C);
}

void omp_gemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
#pragma omp parallel for
	for (int sj = 0; sj < n; sj += BLOCKSIZE)
		for (int si = 0; si < n; si += BLOCKSIZE)
			for (int sk = 0; sk < n; sk += BLOCKSIZE)
				do_block(n, si, sj, sk, A, B, C);
}

void main()
{
	REAL_T* A, * B, * C;
	clock_t start, stop;
	int n = 1024;
	A = new REAL_T[n * n];
	B = new REAL_T[n * n];
	C = new REAL_T[n * n];
	initMatrix(n, A, B, C);

	cout << "origin caculation begin...\n";
	start = clock();
	dgemm(n, A, B, C);
	stop = clock();
	cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
	printFlops(n, n, n, start, stop);

	initMatrix(n, A, B, C);
	cout << "AVX caculation begin...\n";
	start = clock();
	avx_dgemm(n, A, B, C);
	stop = clock();
	cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
	printFlops(n, n, n, start, stop);

	initMatrix(n, A, B, C);
	cout << "parallel AVX caculation begin...\n";
	start = clock();
	pavx_dgemm(n, A, B, C);
	stop = clock();
	cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
	printFlops(n, n, n, start, stop);

	initMatrix(n, A, B, C);
	cout << "blocked AVX caculation begin...\n";
	start = clock();
	block_gemm(n, A, B, C);
	stop = clock();
	cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
	printFlops(n, n, n, start, stop);

	initMatrix(n, A, B, C);
	cout << "OpenMP blocked AVX caculation begin...\n";
	start = clock();
	omp_gemm(n, A, B, C);
	stop = clock();
	cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
	printFlops(n, n, n, start, stop);
}
```



## 2.2 编译器运行

​	根据老师给出的源代码文件，我们将代码放入编译器中进行运行。下面是矩阵规模为$1024\times 1024$的时候的运行结果。

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240427110247407.png" alt="image-20240427110247407"  />

​	同理，我们再进行实验，得到$2048\times 2048$、$3072\times 3072$、$4096\times 4096$的矩阵规模的时间统计。

$2048\times 2048$：

![image-20240427111744168](E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240427111744168.png)

$3072\times 3072$：

![image-20240427112925072](E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240427112925072.png)

$4096\times 4096$：

![image-20240427125320449](E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240427125320449.png)



## 2.3 测试结果

​	我们根据测试得到的时间做出统计，给出以下的柱状图，我们分为运行时间和`GFLOPS`这两个指标来评价我们的性能。

### 2.3.1 运行时间

#### 2.3.1.1 $1024\times 1024$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240427131736547.png" alt="image-20240427131736547" style="zoom: 67%;" />

#### 2.3.1.2 $2048\times 2048$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240427131918746.png" alt="image-20240427131918746" style="zoom:67%;" />

#### 2.3.1.3 $3072\times 3072$

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240427132006111.png" alt="image-20240427132006111" style="zoom:67%;" />

#### 2.3.1.4 $4096\times 4096$

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240427132101842.png" alt="image-20240427132101842" style="zoom:67%;" />



### 2.3.2 GFLOPS

#### 2.3.2.1 $1024\times 1024$

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428090001847.png" alt="image-20240428090001847" style="zoom: 67%;" />

#### 2.3.2.2 $2048\times 2048$

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428090202444.png" alt="image-20240428090202444" style="zoom:67%;" />

#### 2.3.2.3 $3072\times 3072$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428090259762.png" alt="image-20240428090259762" style="zoom:67%;" />

#### 2.3.2.4 $4096\times 4096$

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428090347009.png" alt="image-20240428090347009" style="zoom:67%;" />



## 2.4 归纳与总结

​	通过以上的时间统计，我们发现了一些规律：

1. 从`blocked AVX`到`OpenMP blocked AVX`的优化，起到的作用比较小，都是优化了没几秒，甚至在$2048$矩阵乘法的时候性能还变差了，说明在$1024-4096$这个数量级的矩阵乘法中，这一步优化实际上没有起到很大的作用。
2. 我们发现，随着矩阵规模的增长，时间呈非线性增长，比如说，从$1024$到$2048$，理论上时间应该是乘上$8$，但是实际上时间却变大了$22$左右，与理论值还是有一定的差距的，说明一定还有一些别的影响因素在影响着这个矩阵计算。
3. 我们发现，在从`origin`到`AVX`的优化中，提升的效率是最高的，这说明我们从普通运算到子字并行的提升是最大的，从子字并行到指令集并行。

​	我们从计算耗时、运行性能以及加速比这三个方面来进行优化的评估。

> **计算耗时：**
>
> ​	按照定义计算的矩阵乘法耗时最长，因为它需要执行三重循环来逐个元素进行计算；优化后的矩阵乘法采用分块矩阵乘法的方法，在矩阵乘法计算中减少了不必要的访存操作，从而提高了计算效率，耗时相对较短；并行计算矩阵乘法利用多线程进行计算，可以同时进行多个乘法运算，因此具有更快的计算速度，耗时最短。

> **运行性能：**
>
> ​	按照定义计算的矩阵乘法的性能较低，因为它使用了三重循环的嵌套，导致计算复杂度较高；优化的矩阵乘法通过采用分块矩阵乘法的优化方法，减少了不必要的访存操作，提高了运行性能；并行计算矩阵乘法利用多线程实现并行计算，充分利用多核处理器的计算能力，因此具有更好的运行性能。

> **加速比：**
>
> ​	优化的矩阵乘法和并行计算矩阵乘法相对于按照定义计算的矩阵乘法都能够取得较好的加速比；优化的矩阵乘法通过减少不必要的访存操作和利用分块矩阵乘法的优化策略，加速比相对较高；并行计算矩阵乘法通过利用多线程并行计算的特点，能够进一步提高计算速度，加速比最高。



## 2.5 遇到的问题及解决方法

​	首先，我们根据大一时学习的C++编程的思想，简单的用三层嵌套的`for`循环来实现这个矩阵乘法，但是这样计算的话，时间复杂度会非常高，所耗时较长，性能也很差；于是我们考虑使用分块矩阵乘法等优化方法来减少不必要的计算和访存操作，从而提高计算效率。

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428090758579.png" alt="image-20240428090758579" style="zoom:50%;" />

​	上图就是我们使用子字并行的计算原理，可以发现在这一步我们所提升的性能与加速都是最好的，因为我们同时计算了四个数，相当于并行了$4$核计算，相当于将速度提升了$4$倍左右，但是实际上可能提升仅仅只有两倍左右。

​	接下来我们就使用理论课上讲授的流水线并行，指令集并行的`GEMM`来进行优化。

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428092725897.png" alt="image-20240428092725897" style="zoom:50%;" />

​	我们使用`Unroll`循环展开次数，控制指令依次发射执行理论上至少有$1$倍的加速比，对照前面的结果，确实加速比来到了一倍左右。

​	然后我们再进一步优化，利用理论课第五章的内容，使用`cache`命中来加速，在一个子矩阵（块）被`cache`替换出去之前，最大限度的对其进行数据访问利用时间局部性，提高`cache`命中率，注意到此处，我们需要通过实验和测试来选择合适的分块大小，以达到最佳性能。

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428092928428.png" alt="image-20240428092928428" style="zoom:50%;" />

​	最后，我们利用并行程序设计的思想，将该程序使用4个处理器利用4线程并行来进行加速，但是发现该部分提升的时间和性能并不是特别多，并行计算可能会引发数据竞争和同步问题，导致结果错误或性能下降。所以我们可以使用线程同步机制，如互斥锁（`mutex`）、信号量（`semaphore`）等来解决数据竞争和同步问题。确保每个线程访问共享资源时的互斥和同步操作，以保证正确的计算结果和高效的并行计算。

​	经过优化发现性能仍然不高，甚至有几组数据的`openmp`优化的效率还不如前面的分块运算，所以我们初步认为前面的`cache`优化和流水线优化已经能够很好的优化该计算了。



# 三、Taishan服务器的实验

## 3.1 源代码

```c++
#include <iostream>
#include <ctime>
#include <omp.h>

#define REAL_T double

void printFlops(int A_height, int B_width, int B_height, clock_t start, clock_t stop) {
    REAL_T flops = (2.0 * A_height * B_width * B_height) / 1E9 / ((stop - start) / (CLOCKS_PER_SEC * 1.0));
    std::cout << "GFLOPS:\t" << flops << std::endl;
}

void initMatrix(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
    for (int i = 0; i < n; ++i)
        for (int j = 0; j < n; ++j) {
            A[i + j * n] = (i + j + (i * j) % 100) % 100;
            B[i + j * n] = ((i - j) * (i - j) + (i * j) % 200) % 100;
            C[i + j * n] = 0;
        }
}

// 1. 原始GEMM
void dgemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
    for (int i = 0; i < n; ++i)
        for (int j = 0; j < n; ++j) {
            REAL_T cij = C[i + j * n];
            for (int k = 0; k < n; k++) {
                cij += A[i + k * n] * B[k + j * n];
            }
            C[i + j * n] = cij;
        }
}

// 2. 指令集并行优化（ILP）
void ILP_dgemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
    // 将内层循环拆分为多个小循环，以提高指令级并行性
    for (int i = 0; i < n; ++i)
        for (int j = 0; j < n; ++j) {
            REAL_T cij = C[i + j * n];
            for (int k = 0; k < n; k += 4) { // 假设处理器支持4个并行乘法
                cij += A[i + k * n] * B[k + j * n];
                cij += A[i + (k + 1) * n] * B[(k + 1) + j * n];
                cij += A[i + (k + 2) * n] * B[(k + 2) + j * n];
                cij += A[i + (k + 3) * n] * B[(k + 3) + j * n];
            }
            C[i + j * n] = cij;
        }
}

// 3. 考虑Cache的分块优化
void block_gemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
    int BLOCK_SIZE = 64; // 块大小
    for (int bi = 0; bi < n; bi += BLOCK_SIZE)
        for (int bj = 0; bj < n; bj += BLOCK_SIZE)
            for (int bk = 0; bk < n; bk += BLOCK_SIZE)
                for (int i = bi; i < std::min(n, bi + BLOCK_SIZE); ++i)
                    for (int j = bj; j < std::min(n, bj + BLOCK_SIZE); ++j) {
                        REAL_T cij = C[i + j * n];
                        for (int k = bk; k < std::min(n, bk + BLOCK_SIZE); ++k) {
                            cij += A[i + k * n] * B[k + j * n];
                        }
                        C[i + j * n] = cij;
                    }
}

#define BLOCKSIZE 64
void do_block(int n, int si, int sj, int sk, REAL_T* A, REAL_T* B, REAL_T* C) {
    for (int i = si; i < si + BLOCKSIZE; ++i)
        for (int j = sj; j < sj + BLOCKSIZE; ++j) {
            REAL_T cij = C[i + j * n];
            for (int k = sk; k < sk + BLOCKSIZE; ++k) {
                cij += A[i + k * n] * B[k + j * n];
            }
            C[i + j * n] = cij;
        }
}

// 4. 多处理器并行的优化
void omp_gemm(int n, REAL_T* A, REAL_T* B, REAL_T* C) {
#pragma omp parallel for
    for (int sj = 0; sj < n; sj += BLOCKSIZE)
        for (int si = 0; si < n; si += BLOCKSIZE)
            for (int sk = 0; sk < n; sk += BLOCKSIZE)
                do_block(n, si, sj, sk, A, B, C);
}
int main() {
    REAL_T* A, * B, * C;
    clock_t start, stop;
    int n = 4096;//1024-4096修改

    A = new REAL_T[n * n];
    B = new REAL_T[n * n];
    C = new REAL_T[n * n];
    initMatrix(n, A, B, C);

    std::cout << "origin calculation begin...\n";
    start = clock();
    dgemm(n, A, B, C);
    stop = clock();
    std::cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
    printFlops(n, n, n, start, stop);

    initMatrix(n, A, B, C);
    std::cout << "ILP calculation begin...\n";
    start = clock();
    ILP_dgemm(n, A, B, C);
    stop = clock();
    std::cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
    printFlops(n, n, n, start, stop);

    initMatrix(n, A, B, C);
    std::cout << "blocked calculation begin...\n";
    start = clock();
    block_gemm(n, A, B, C);
    stop = clock();
    std::cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
    printFlops(n, n, n, start, stop);

    initMatrix(n, A, B, C);
    std::cout << "OpenMP blocked calculation begin...\n";
    start = clock();
    omp_gemm(n, A, B, C);
    stop = clock();
    std::cout << (stop - start) / CLOCKS_PER_SEC << "." << (stop - start) % CLOCKS_PER_SEC << "\t\t";
    printFlops(n, n, n, start, stop);

    delete[] A;
    delete[] B;
    delete[] C;

    return 0;
}
```



## 3.2 Taishan服务器运行

​	首先我们根据老师给出的服务器端口来连接系统，输入账号`stu2211044`和密码`123456`就可以进入系统了。

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428101127026.png" alt="image-20240428101127026" style="zoom: 50%;" />

​	输入命令行`gcc -v`和`g++ -v`来检验系统是否安装编译环境，发现输出了对应的版本号$9.4.0$，说明系统中存在我们所需要的编译环境。

![image-20240428101557249](E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428101557249.png)

​	我们在本地创建自己的目录，获取到了我们的代码存放的位置：`/home/stu2211044/code`，接下来我们配置我们的编译环境，结果如下所示：可以看到我们已经完成了自己的编译环境的搭建！

​	然后我们通过`vim`和`g++`来进行代码的编辑与编译运行，就可以得到最后的结果了，我们修改$n$的值，分别计算出$1024-4096$的不同结果。

​	第一个文件`final.cpp`跑出来的结果如下所示：

![image-20240428185013708](E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428185013708.png)

​	第二个文件`final2.cpp`跑出来的结果如下所示：

![image-20240428190202122](E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428190202122.png)

​	第三个文件`final3.cpp`跑出来的结果如下所示：

![image-20240428193702480](E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428193702480.png)

​	第四个文件`final4.cpp`跑出来的结果如下所示：

![image-20240428235400195](E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428235400195.png)

​	因为此处运行时间过长，所以我将它放到了后台运行，最后使用命令行`cat nohup.out`来查看最后的结果。



## 3.3 测试结果

### 3.3.1 运行时间

#### 3.3.1.1  $1024\times 1024$

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428202222052.png" alt="image-20240428202222052" style="zoom:67%;" />

#### 3.3.1.2  $2048\times 2048$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428202412211.png" alt="image-20240428202412211" style="zoom:67%;" />

#### 3.3.1.3  $3072\times 3072$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428202611594.png" alt="image-20240428202611594" style="zoom:67%;" />

#### 3.3.1.4  $4096\times 4096$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428235604297.png" alt="image-20240428235604297" style="zoom:67%;" />



### 3.3.2 GFLOPS

#### 3.3.2.1  $1024\times 1024$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428203011051.png" alt="image-20240428203011051" style="zoom:67%;" />

#### 3.3.2.2  $2048\times 2048$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428202910250.png" alt="image-20240428202910250" style="zoom:67%;" />

#### 3.3.2.3  $3072\times 3072$​

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428202758113.png" alt="image-20240428202758113" style="zoom:67%;" />

#### 3.3.2.4  $4096\times 4096$

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428235648300.png" alt="image-20240428235648300" style="zoom:67%;" />



## 3.4 归纳与总结

​	通过以上的时间统计，我们发现了一些规律：

1. 我们发现使用`taishan`服务器运行得出的结果并没有比在`PC`电脑上得出的结果要好，反而性能还差于`PC`电脑。
2. 在性能的替身方面，也与`PC`电脑端差距较大：`PC`端提升较快的是第一级到第二级，而`taishan`服务器提升较快的是第二级到第三级，有一定的差距。
3. 在`OpenMP`的使用方面，`taishan`服务器的表现要明显优于`PC`端，这是因为在服务器上运行并行程序会大大加速程序的速度，而在个人`PC`上就不一定会加速很多了。

我们从计算耗时、运行性能以及加速比这三个方面来进行优化的评估。

> **计算耗时：**
>
> ​	按照定义计算的矩阵乘法耗时最长，因为它需要执行三重循环来逐个元素进行计算；优化后的矩阵乘法采用分块矩阵乘法的方法，在矩阵乘法计算中减少了不必要的访存操作，从而提高了计算效率，耗时相对较短；并行计算矩阵乘法利用多线程进行计算，可以同时进行多个乘法运算，因此具有更快的计算速度，耗时最短。

> **运行性能：**
>
> ​	按照定义计算的矩阵乘法的性能较低，因为它使用了三重循环的嵌套，导致计算复杂度较高；优化的矩阵乘法通过采用分块矩阵乘法的优化方法，减少了不必要的访存操作，提高了运行性能；并行计算矩阵乘法利用多线程实现并行计算，充分利用多核处理器的计算能力，因此具有更好的运行性能。

> **加速比：**
>
> ​	优化的矩阵乘法和并行计算矩阵乘法相对于按照定义计算的矩阵乘法都能够取得较好的加速比；优化的矩阵乘法通过减少不必要的访存操作和利用分块矩阵乘法的优化策略，加速比相对较高；并行计算矩阵乘法通过利用多线程并行计算的特点，能够进一步提高计算速度，加速比最高。在服务器上的表现是明显要优于个人电脑的。



## 3.5 遇到的问题及解决方法

​	遇到的问题有挺多的，比如说刚开始的配`taishan`环境，以及后面的运行得出的结果，都遇到了很多的困难，下面我一一列举：

​	首先就是对于环境的配置，我先根据教程打开`putty`，然后输入`ip`地址，就进入了一个命令行输入的一个界面。我是第一次自己编写`linux`命令行语句来操作，所以并不是很熟悉，我也在网上查了很多教程，但是还是无法进入我的`home`界面，然后我就询问了老师，老师也给我了简洁清晰的回答。

<img src="E:\学学学\本科\大二下\计算机组成原理\实验报告_2211044_陆皓喆\大作业_矩阵乘法\陆皓喆_2211044_组成原理矩阵乘法作业.assets\image-20240428205643002.png" alt="image-20240428205643002" style="zoom: 50%;" />

​	最后根据老师的说明，我成功的找到了我的`home`地址。接下来我就准备将修改好的代码进行运行，但是一时间不知道如何将代码导入到我的服务器端。我在网上查询了很多资料，有的说需要下载一个`fxtp`和`xshell`来传输文件，有的说不用下载，直接传输就可以了。最后，我仔细阅读参考资料，最后使用在服务器自己利用`vim`语句建立一个文件，再使用`g++`来进行编译的方法，最后解决了这个问题。这次实验，让我初步了解了`linux`语句的使用，也扩展了我的知识面。



# 四、PC与taishan服务器的差异

​	原先我觉得，个人`PC`端跑出来的结果一定会差于`taishan`服务器端口，但是事实上并不是这样的。我发现在所有的测试点上，`PC`基本都是要优于`taishan`服务器的，不管是在时长上，还是在性能方面，`taishan`服务器的性能居然上限就只有$0.27$左右，但是`PC`端却可以达到$2$以上，这也颠覆了我的认知。

​	在提升加速比这一块，我发现了：个人`PC`在第一级的提升是最快的，但是在并行这一级的提升接近$0$，有的甚至还倒退了；`taishan`服务器的话，在第二级到第三级的流水线是提升最快的一级，但是在第一级的提升就要明显弱于`PC`，我猜测是其巨大的算力导致无法进行四核运算（不太确定），但是在并行这一级，我们的服务器提升是巨大的，几乎有了接近一倍的提升，这也与我们的`PC`不太相同。



# 五、总结感想

​	通过此次实验，我了解了个人`PC`与`taishan`服务器的不同，深入了解了矩阵乘法的逐级优化的原理与上手实操，了解了`linux`命令行的语句编写，更深入熟悉了性能，时间，加速比等基础概念，了解到了两者的差异与共同之处，学会了使用`putty`来进行`SSH`的客户端连接等等。这次作业很好的将理论课上所学习的流水线、`cache`缓存加速、并行程序加速等知识点与实验结合在了一起，是一次很好的锻炼，希望以后有更多这样的实验。
